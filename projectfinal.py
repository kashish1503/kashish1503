# -*- coding: utf-8 -*-
"""projectFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rab5bAl7cWcmNo_ZkrMQkGR_uH8WXRhw
"""

!pip install opencv-contrib-python
!pip install diffusers transformers accelerate

import torch
import os
from huggingface_hub import HfApi
from pathlib import Path
from diffusers.utils import load_image
import numpy as np
import cv2
from PIL import Image

from diffusers import (
    ControlNetModel,
    StableDiffusionControlNetPipeline,
    UniPCMultistepScheduler,
)

"""<h2> Captions </h2>"""

import os
import torch
from diffusers import StableDiffusionPipeline
from PIL import Image

# Load the Stable Diffusion model pipeline
model_name = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_name, torch_dtype=torch.float16)
pipe = pipe.to("cuda" if torch.cuda.is_available() else "cpu")

# Define the caption for image generation
caption1 = "A couple watches a boat against a skyline"
caption2 = "The girl in the white strip is falling down as the girl in the blue strip challenges for the soccer ball"
caption3 = "A hiker is pointing towards the mountains"
caption4 = "Two kids play hockey in the snow"
caption5 = "A boat on the water"
caption6 = "A water bird standing at the ocean's edge"

# Generate the image
image = pipe(caption1).images[0]

# Save the generated image
output_path = "generated_image1.png"
image.save(output_path)
print(f"Image generated and saved to {output_path}")

"""<h2> Captions + Canny Edge </h2>"""

checkpoint = "lllyasviel/control_v11p_sd15_canny"

# image = load_image("https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png")

# image = np.array(image)

# low_threshold = 100
# high_threshold = 200

# image = cv2.Canny(image, low_threshold, high_threshold)
# image = image[:, :, None]
# image = np.concatenate([image, image, image], axis=2)
# control_image = Image.fromarray(image)

# control_image.save("/content/control.png")

controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
  "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

generator = torch.manual_seed(33)
# image = pipe("a blue paradise bird in the jungle", num_inference_steps=20, generator=generator, image=control_image).images[0]

# image.save('/content/image_out.png')

"""<h3> For single image </h3>

"""

image = load_image("/content/99679241_adc853a5c0.jpg")

lr_image = load_image("/content/90011335_cfdf9674c2.jpg")
new_size = (128,128)
# lr_image = lr_image.resize(new_size)
# image = image.resize(new_size)
print("type_input: ", type(image))
print("INPUT")
print("high_res: ")
print("Image_reso: ", image.size)
display(image)

# print("low_res: ")
# print("Image_reso: ", lr_image.size)
# display(lr_image)

image = np.array(image)

low_threshold = 100
high_threshold = 200

can_image = cv2.Canny(image, low_threshold, high_threshold)
can_image = can_image[:, :, None]
can_image = np.concatenate([can_image, can_image, can_image], axis=2)
control_image = Image.fromarray(can_image)
print("type_canny: ",type(control_image))
control_image.save("/content/control1.png")

image = pipe("A water bird standing at the ocean's edge", num_inference_steps=20, generator=generator, image=control_image).images[0]

image.save('/content/image_out.png')
print("OUTPUT")
print("before_resize: ")
display(image)
image = image.resize(new_size)
print("after_resize: ")
print("image_resolution: ", image.size)
display(image)

"""<h3> For multiple Image </h3>"""

from google.colab import drive
drive.mount('/content/drive')

metadata={}
with open('/content/drive/MyDrive/best_captions.txt', 'r') as f:
  for line in f:
    file, text = line.strip().split(',', 1)
    metadata[file] = text

count=0
for file in os.listdir('/content/drive/MyDrive/Images'):
  if count==2:
    break
  low_image=Image.open('/content/drive/MyDrive/lowres100/'+'low_res_'+file)
  hr_image=Image.open('/content/drive/MyDrive/High_images/'+file)
  low_image_np = np.array(low_image)
  hr_image_np = np.array(hr_image)

  # Apply Canny edge detection
  canny_image = cv2.Canny(hr_image_np, low_threshold, high_threshold)
  canny_image = canny_image[:, :, None]
  canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)
  control_image = Image.fromarray(canny_image)

  # Save control image (optional, for debugging)
  control_image.save(os.path.join(control_folder, f"control_{idx + 1}.png"))
  text=metadata[file]
  low_image = low_image.resize((128, 128))
  # line_art = line_art.resize(low_image.size)
  low_image_array = np.array(low_image)
  # line_art_array = np.array(line_art)
  # low_image_tensor = torch.tensor(low_image_array).float().unsqueeze(0).permute(0, 3, 1, 2).to('cuda')
  # line_art_tensor = torch.tensor(line_art_array).float().unsqueeze(0).permute(0, 3, 1, 2).to('cuda')
  output = pipe(text, image=low_image, control_image=control_image).images[0]
  print("caption: ", text)
  print("size_li: ", low_image.size)
  print("size_la: ", control_image.size)
  print("size_o: ", output.size)
  display(low_image)
  display(control_image)
  display(output)
  output.save('/content/drive/MyDrive/output/'+'out_'+file)
  count+=1

"""*italicised text*<h2> Caption + Line Art </h2>

checkpoint = "ControlNet-1-1-preview/control_v11p_sd15_lineart"
"""

!pip install controlnet_aux==0.0.9

from controlnet_aux import LineartDetector

from diffusers import (
    ControlNetModel,
    StableDiffusionControlNetPipeline,
    UniPCMultistepScheduler,
)

checkpoint = "ControlNet-1-1-preview/control_v11p_sd15_lineart"

# image = load_image(
#     "https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png"
# )
# image = image.resize((512, 512))

# prompt = "a black girl with light brown hairs"
# processor = LineartDetector.from_pretrained("lllyasviel/Annotators")

# control_image = processor(image)
# control_image.save("/content/control.png")

controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

generator = torch.manual_seed(0)
# image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]

# image.save('/content/image_out.png')

image = load_image(
    "/content/99679241_adc853a5c0.jpg"
)

lr_image = load_image("/content/99679241_adc853a5c0.jpg")
new_size = (128,128)
# lr_image = lr_image.resize(new_size)
# image = image.resize(new_size)
print("type_input: ", type(image))
print("INPUT")
print("high_res: ")
print("Image_reso: ", image.size)
display(image)

print("low_res: ")
print("Image_reso: ", lr_image.size)
# display(lr_image)

processor = LineartDetector.from_pretrained("lllyasviel/Annotators")

control_image = processor(image)
control_image.save("/content/control.png")

# control_image = load_image("/content/17273391_55cfc7d3d4_line_art.jpg")
# print("type_canny: ",type(control_image))
control_image.save("/content/control1.png")

image = pipe("A water bird standing at the ocean's edge", num_inference_steps=20, generator=generator, control_image=image, image=control_image).images[0]

image.save('/content/image_out.png')
print("OUTPUT")
print("before_resize: ")
print("image_resolution: ", image.size)
display(image)
image = image.resize(new_size)
print("after_resize: ")
print("image_resolution: ", image.size)
display(image)

"""<h2> Caption + Depth Map </h2>"""

!pip install diffusers transformers accelerate
!pip install xformers

from transformers import pipeline
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from PIL import Image
import numpy as np
import torch
from diffusers.utils import load_image

depth_estimator = pipeline('depth-estimation')

image = load_image("https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png")

image = depth_estimator(image)['depth']
image = np.array(image)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
image = Image.fromarray(image)

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-depth", torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# Remove if you do not have xformers installed
# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers
# for installation instructions
# pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

image = pipe("Stormtrooper's lecture", image, num_inference_steps=20).images[0]

image.save('/content/stormtrooper_depth_out.png')

image = load_image("/content/47871819_db55ac4699.jpg")

# lr_image = load_image("/content/99679241_adc853a5c0.jpg")
new_size = (128,128)
# lr_image = lr_image.resize(new_size)
# image = image.resize(new_size)
print("type_input: ", type(image))
print("INPUT")
print("high_res: ")
print("Image_reso: ", image.size)
display(image)

# print("low_res: ")
# print("Image_reso: ", lr_image.size)
# display(lr_image)

prompt = "The girl in the white strip is falling down as the girl in the blue strip challenges for the soccer ball"

depth_estimator = pipeline('depth-estimation')
image = depth_estimator(image)['depth']
image = np.array(image)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
control_image = Image.fromarray(image)

control_image.save("/content/control.png")

image = pipe(prompt, num_inference_steps=20, control_image=image, image=control_image).images[0]

image.save('/content/image_out.png')
print("OUTPUT")
print("before_resize: ")
print("image_resolution: ", image.size)
display(image)
image = image.resize(new_size)
print("after_resize: ")
print("image_resolution: ", image.size)
display(image)

"""<h2> SSIM </h2>"""

import numpy as np
from skimage.metrics import mean_squared_error
from skimage.io import imread
from skimage.transform import resize
from skimage.color import gray2rgb

def calculate_image_mse(image_true, image_pred):
    """
    Calculate the Mean Squared Error (MSE) between two images.

    Parameters:
    image_true (numpy array): The ground truth image (actual values).
    image_pred (numpy array): The predicted/generated image.

    Returns:
    float: The Mean Squared Error.
    """
    # Ensure the images have the same shape
    if image_true.shape != image_pred.shape:
        raise ValueError("The input images must have the same dimensions.")

    # Compute MSE using scikit-image's built-in function
    mse = mean_squared_error(image_true, image_pred)
    return mse

if __name__ == "__main__":
    # Paths to the ground truth and predicted images
    image_true_path = "/content/99679241_adc853a5c0.jpg"
    image_pred_path = "/content/gen6(1).png"

    # Read the images from the given paths
    image_true = imread(image_true_path)
    image_pred = imread(image_pred_path)

    print("Original shapes:")
    print("True image shape:", image_true.shape)
    print("Predicted image shape:", image_pred.shape)

    # Resize predicted image to match the true image's dimensions
    image_pred_resized = resize(image_pred, image_true.shape, anti_aliasing=True)
    image_pred_resized = (image_pred_resized * 255).astype(image_true.dtype)

    # Ensure shapes match
    if image_true.shape != image_pred_resized.shape:
        raise ValueError("The resized image dimensions do not match.")

    # Calculate the MSE
    mse = calculate_image_mse(image_true, image_pred_resized)
    print(f"Mean Squared Error between images: {mse}")

"""<h2> MSE </h2>"""

import numpy as np
from skimage.metrics import mean_squared_error
from skimage.io import imread
from skimage.transform import resize
from skimage.color import gray2rgb

def calculate_image_mse(image_true, image_pred):
    """
    Calculate the Mean Squared Error (MSE) between two images.

    Parameters:
    image_true (numpy array): The ground truth image (actual values).
    image_pred (numpy array): The predicted/generated image.

    Returns:
    float: The Mean Squared Error.
    """
    # Ensure the images have the same shape
    if image_true.shape != image_pred.shape:
        raise ValueError("The input images must have the same dimensions.")

    # Compute MSE using scikit-image's built-in function
    mse = mean_squared_error(image_true, image_pred)
    return mse

if __name__ == "__main__":
    # Paths to the ground truth and predicted images
    image_true_path = "/content/44129946_9eeb385d77.jpg"
    image_pred_path = "/content/gen1(1).png"

    # Read the images from the given paths
    image_true = imread(image_true_path)
    image_pred = imread(image_pred_path)

    print("Original shapes:")
    print("True image shape:", image_true.shape)
    print("Predicted image shape:", image_pred.shape)

    # Resize predicted image to match the true image's dimensions
    image_pred_resized = resize(image_pred, image_true.shape, anti_aliasing=True)
    image_pred_resized = (image_pred_resized * 255).astype(image_true.dtype)

    # Ensure shapes match
    if image_true.shape != image_pred_resized.shape:
        raise ValueError("The resized image dimensions do not match.")

    # Calculate the MSE
    mse = calculate_image_mse(image_true, image_pred_resized)
    print(f"Mean Squared Error between images: {mse}")

"""<h2> PSNR </h2>"""

import numpy as np
import os
from skimage.metrics import mean_squared_error, peak_signal_noise_ratio
from skimage.io import imread
from skimage.transform import resize
from skimage.color import gray2rgb

def calculate_image_mse(image_true, image_pred):
    if image_true.shape != image_pred.shape:
        raise ValueError("The input images must have the same dimensions.")
    return mean_squared_error(image_true, image_pred)

def calculate_image_psnr(image_true, image_pred):
    if image_true.shape != image_pred.shape:
        raise ValueError("The input images must have the same dimensions.")
    return peak_signal_noise_ratio(image_true, image_pred, data_range=image_true.max() - image_true.min())

def calculate_metrics_for_single_image(true_image_path, pred_image_path):
    if not os.path.exists(true_image_path):
        raise FileNotFoundError(f"Ground truth image not found: {true_image_path}")
    if not os.path.exists(pred_image_path):
        raise FileNotFoundError(f"Predicted image not found: {pred_image_path}")

    image_true = imread(true_image_path)
    image_pred = imread(pred_image_path)

    # Ensure the images are in the same format
    if len(image_true.shape) == 2:
        image_true = gray2rgb(image_true)
    if len(image_pred.shape) == 2:
        image_pred = gray2rgb(image_pred)

    # Resize predicted image to match the ground truth dimensions
    if image_true.shape != image_pred.shape:
        image_pred = resize(image_pred, image_true.shape, anti_aliasing=True)
        image_pred = (image_pred * 255).astype(image_true.dtype)

    mse = calculate_image_mse(image_true, image_pred)
    psnr = calculate_image_psnr(image_true, image_pred)

    return {"MSE": mse, "PSNR": psnr}

# # Example usage
# if __name__ == "__main__":
#     true_image_path = "/content/44129946_9eeb385d77.jpg"
#     pred_image_path = "/content/image1.png"

"""<h2> LPIPS </h2>"""

!pip install lpips

import lpips
import torch
from PIL import Image
from torchvision import transforms

def calculate_lpips_score(true_image_path, pred_image_path, use_gpu=False):
    """
    Calculate the LPIPS score between two images.

    Parameters:
    true_image_path (str): Path to the ground truth image.
    pred_image_path (str): Path to the predicted/generated image.
    use_gpu (bool): Whether to use GPU for computation.

    Returns:
    float: The LPIPS score.
    """
    # Load the LPIPS model
    lpips_model = lpips.LPIPS(net='alex')  # Using AlexNet as the backbone
    if use_gpu and torch.cuda.is_available():
        lpips_model = lpips_model.cuda()

    # Define image transformation
    transform = transforms.Compose([
        transforms.Resize((256, 256)),  # Resize to a fixed size
        transforms.ToTensor(),         # Convert to tensor
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]
    ])

    # Load and transform images
    true_image = Image.open(true_image_path).convert("RGB")
    pred_image = Image.open(pred_image_path).convert("RGB")
    true_image = transform(true_image).unsqueeze(0)  # Add batch dimension
    pred_image = transform(pred_image).unsqueeze(0)

    # Move to GPU if available and requested
    if use_gpu and torch.cuda.is_available():
        true_image = true_image.cuda()
        pred_image = pred_image.cuda()

    # Compute LPIPS score
    lpips_score = lpips_model(true_image, pred_image)
    return lpips_score.item()

# Example usage
if __name__ == "__main__":
    true_image_path = "/content/90011335_cfdf9674c2.jpg"
    pred_image_path = "/content/image_out5.png"

    metrics = calculate_metrics_for_single_image(true_image_path, pred_image_path)
    print(f"MSE = {metrics['MSE']}, PSNR = {metrics['PSNR']} dB")

    score = calculate_lpips_score(true_image_path, pred_image_path, use_gpu=True)
    print(f"LPIPS Score: {score}")

